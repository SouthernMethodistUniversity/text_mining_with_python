{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate TextMining with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " By: Dr. Eric Godat and Dr. Rob Kalescky\n",
    " \n",
    " Acknowledgements: Garrett Moore, Jaymie Ruddock\n",
    "\n",
    " Adapted from: [Ultimate Guide to deal with Text Data (Using Python)](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/)\n",
    " \n",
    " Natural Language Toolkit: [Documentation](http://www.nltk.org/)\n",
    " \n",
    " Reference Text: [Natural Language Processing with Python](http://www.nltk.org/book/)\n",
    " \n",
    " \n",
    " *This notebook follows the **Introduction to Working with Text Data** lesson. Please consider looking over that lesson if you have not already*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the basic libraries we will use in for data manipulation (pandas) and math functions (numpy). We will add more libraries as we need them.\n",
    "\n",
    "As a best practice, it is a good idea to load all your libraries in a single cell at the top of a notebook, however for the purposes of this tutorial we will load some now and more as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDGAR Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the [homepage](https://www.sec.gov/edgar.shtml) for the Securities and Exchange Commission's EDGAR database. If you are having trouble finding a specific company, try their [full text search](https://www.sec.gov/edgar/search/#).\n",
    "\n",
    "Usage documentation for the Python package can be found [here](https://pypi.org/project/edgar/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the EDGAR python package and define a function that pulls 10-K filings for our companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edgar import Company, TXTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edgar(ll, n):\n",
    "    filinglist = []\n",
    "    for el in ll:\n",
    "        company = Company(el[0], el[2])\n",
    "        tree = company.get_all_filings(filing_type = \"10-K\")\n",
    "        docs = Company.get_documents(tree, no_of_documents=n, as_documents=True)\n",
    "        texts = Company.get_documents(tree, no_of_documents=n, as_documents=False)\n",
    "        if n<2:\n",
    "            docs=[docs]\n",
    "            texts=[texts]\n",
    "        for i in range(n):\n",
    "            date = docs[i].content['Filing Date']\n",
    "            text = TXTML.parse_full_10K(texts[i])\n",
    "            filinglist.append([el[0],el[1],el[2],date,text])\n",
    "    df = pd.DataFrame(filinglist, columns=['Company','Ticker','CIK','Date','Text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our list of companies: \\[Name, Ticker symbol, CIK\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = [\n",
    "['Alphabet Inc.','(GOOG, GOOGL)','0001652044'],\n",
    "['AMAZON COM INC','(AMZN)','0001018724'],\n",
    "['AMERISOURCEBERGEN CORP','(ABC)','0001140859'],\n",
    "['Apple Inc.','(AAPL)','0000320193'],\n",
    "['AT&T INC.','(T, TBC, TBB, T-PC, T-PA)','0000732717'],\n",
    "['BANK OF AMERICA CORP','(BAC, MER-PK, BML-PL, BML-PJ, BML-PH, BML-PG, BAC-PM, BAC-PL, BAC-PK, BAC-PE, BAC-PC, BAC-PB, BAC-PA)','0000070858'],\n",
    "['BERKSHIRE HATHAWAY INC','(BRK-B, BRK-A)','0001067983'],\n",
    "['CARDINAL HEALTH INC','(CAH)','0000721371'],\n",
    "['CHEVRON CORP','(CVX)','0000093410'],\n",
    "['COSTCO WHOLESALE CORP','(COST)','0000909832'],\n",
    "['CVS HEALTH Corp','(CVS)','0000064803'],\n",
    "['EXXON MOBIL CORP','(XOM)','0000034088'],\n",
    "['Facebook Inc','(FB)','0001326801'],\n",
    "['FORD MOTOR CO','(F, F-PC, F-PB)','0000037996'],\n",
    "['General Motors Co','(GM)','0001467858'],\n",
    "['JPMORGAN CHASE & CO','(JPM, AMJ, PPLN, JPM-PJ, JPM-PH, JPM-PG, JPM-PD, JPM-PC)','0000019617'],\n",
    "['MCKESSON CORP','(MCK)','0000927653'],\n",
    "['MICROSOFT CORP','(MSFT)','0000789019'],\n",
    "['UNITEDHEALTH GROUP INC','(UNH)','0000731766'],\n",
    "['Walmart Inc.','(WMT)','0000104169'],\n",
    "['GameStop Corp.','(GME)','0001326380'],\n",
    "['STARBUCKS CORP','(SBUX)','0000829224']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last 3 10Ks with Filing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_edgar(companies,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alphabet Inc.</td>\n",
       "      <td>(GOOG, GOOGL)</td>\n",
       "      <td>0001652044</td>\n",
       "      <td>2021-02-03</td>\n",
       "      <td>\\n\\n\\n\\n\\ngoog-20201231FALSE2020FY0001652044P7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alphabet Inc.</td>\n",
       "      <td>(GOOG, GOOGL)</td>\n",
       "      <td>0001652044</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alphabet Inc.</td>\n",
       "      <td>(GOOG, GOOGL)</td>\n",
       "      <td>0001652044</td>\n",
       "      <td>2019-02-06</td>\n",
       "      <td>\\n\\nAmendment No. 1 to Form 10-K\\n\\n \\n\\n    \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>(AMZN)</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>2021-02-03</td>\n",
       "      <td>\\n\\n\\n\\n\\namzn-20201231false2020FY0001018724P3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMAZON COM INC</td>\n",
       "      <td>(AMZN)</td>\n",
       "      <td>0001018724</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>GameStop Corp.</td>\n",
       "      <td>(GME)</td>\n",
       "      <td>0001326380</td>\n",
       "      <td>2019-04-02</td>\n",
       "      <td>\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t\\nUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>GameStop Corp.</td>\n",
       "      <td>(GME)</td>\n",
       "      <td>0001326380</td>\n",
       "      <td>2018-04-02</td>\n",
       "      <td>\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t\\nUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>STARBUCKS CORP</td>\n",
       "      <td>(SBUX)</td>\n",
       "      <td>0000829224</td>\n",
       "      <td>2020-11-12</td>\n",
       "      <td>\\n\\n\\n\\n\\nsbux-20200927falseTRUE2020FY00008292...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>STARBUCKS CORP</td>\n",
       "      <td>(SBUX)</td>\n",
       "      <td>0000829224</td>\n",
       "      <td>2019-11-15</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>STARBUCKS CORP</td>\n",
       "      <td>(SBUX)</td>\n",
       "      <td>0000829224</td>\n",
       "      <td>2018-11-16</td>\n",
       "      <td>\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t\\nTa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Company         Ticker         CIK        Date  \\\n",
       "0    Alphabet Inc.  (GOOG, GOOGL)  0001652044  2021-02-03   \n",
       "1    Alphabet Inc.  (GOOG, GOOGL)  0001652044  2020-02-04   \n",
       "2    Alphabet Inc.  (GOOG, GOOGL)  0001652044  2019-02-06   \n",
       "3   AMAZON COM INC         (AMZN)  0001018724  2021-02-03   \n",
       "4   AMAZON COM INC         (AMZN)  0001018724  2020-01-31   \n",
       "..             ...            ...         ...         ...   \n",
       "61  GameStop Corp.          (GME)  0001326380  2019-04-02   \n",
       "62  GameStop Corp.          (GME)  0001326380  2018-04-02   \n",
       "63  STARBUCKS CORP         (SBUX)  0000829224  2020-11-12   \n",
       "64  STARBUCKS CORP         (SBUX)  0000829224  2019-11-15   \n",
       "65  STARBUCKS CORP         (SBUX)  0000829224  2018-11-16   \n",
       "\n",
       "                                                 Text  \n",
       "0   \\n\\n\\n\\n\\ngoog-20201231FALSE2020FY0001652044P7...  \n",
       "1   \\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...  \n",
       "2   \\n\\nAmendment No. 1 to Form 10-K\\n\\n \\n\\n    \\...  \n",
       "3   \\n\\n\\n\\n\\namzn-20201231false2020FY0001018724P3...  \n",
       "4   \\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...  \n",
       "..                                                ...  \n",
       "61  \\n\\n\\n\\t\\n\\t\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t\\nUN...  \n",
       "62  \\n\\n\\n\\t\\n\\t\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t\\nUN...  \n",
       "63  \\n\\n\\n\\n\\nsbux-20200927falseTRUE2020FY00008292...  \n",
       "64  \\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...  \n",
       "65  \\n\\n\\n\\t\\n\\t\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t\\nTa...  \n",
       "\n",
       "[66 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words and Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first bit of analysis we might want to do is to count the number of words in one piece of data. To do this we will add a column called *wordcount* and write an operation that applies a function to every row of the column.\n",
    "\n",
    "Unpacking this piece of code, *len(str(x).split(\" \")*, tells us what is happening.\n",
    "\n",
    "For the content of cell *x*, convert it to a string, *str()*, then split that string into pieces at each space, *split()*.\n",
    "\n",
    "The result of that is a list of all the words in the text and then we can count the length of that list, *len()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>wordcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\n\\n\\ngoog-20201231FALSE2020FY0001652044P7...</td>\n",
       "      <td>47494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...</td>\n",
       "      <td>44102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nAmendment No. 1 to Form 10-K\\n\\n \\n\\n    \\...</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\n\\n\\namzn-20201231false2020FY0001018724P3...</td>\n",
       "      <td>34933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...</td>\n",
       "      <td>36029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  wordcount\n",
       "0  \\n\\n\\n\\n\\ngoog-20201231FALSE2020FY0001652044P7...      47494\n",
       "1  \\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...      44102\n",
       "2  \\n\\nAmendment No. 1 to Form 10-K\\n\\n \\n\\n    \\...       1851\n",
       "3  \\n\\n\\n\\n\\namzn-20201231false2020FY0001018724P3...      34933\n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...      36029"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['wordcount'] = data['Text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "data[['Text','wordcount']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do something similar to count the number of characters in the data chunk, including spaces. If you wanted to exclude whitespaces, you could take the list we made above, join it together and count the length of the resulting string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(\"No Information Provided\") #If some of our data is missing, this will replace the blank entries. This is only necessary in some cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\n\\n\\ngoog-20201231FALSE2020FY0001652044P7...</td>\n",
       "      <td>375336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...</td>\n",
       "      <td>364302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nAmendment No. 1 to Form 10-K\\n\\n \\n\\n    \\...</td>\n",
       "      <td>14992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\n\\n\\namzn-20201231false2020FY0001018724P3...</td>\n",
       "      <td>274004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...</td>\n",
       "      <td>305025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  char_count\n",
       "0  \\n\\n\\n\\n\\ngoog-20201231FALSE2020FY0001652044P7...      375336\n",
       "1  \\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...      364302\n",
       "2  \\n\\nAmendment No. 1 to Form 10-K\\n\\n \\n\\n    \\...       14992\n",
       "3  \\n\\n\\n\\n\\namzn-20201231false2020FY0001018724P3...      274004\n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...      305025"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['char_count'] = data['Text'].str.len() ## this also includes spaces, to do it without spaces, you could use something like this: \"\".join()\n",
    "data[['Text','char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to calculate the average word length in the data.\n",
    "\n",
    "Let's define a function that will do that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply that function to all the data chunks and save that in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\n\\n\\ngoog-20201231FALSE2020FY0001652044P7...</td>\n",
       "      <td>6.608880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...</td>\n",
       "      <td>6.342718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nAmendment No. 1 to Form 10-K\\n\\n \\n\\n    \\...</td>\n",
       "      <td>5.301764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\n\\n\\namzn-20201231false2020FY0001018724P3...</td>\n",
       "      <td>6.652604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...</td>\n",
       "      <td>6.460655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  avg_word\n",
       "0  \\n\\n\\n\\n\\ngoog-20201231FALSE2020FY0001652044P7...  6.608880\n",
       "1  \\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...  6.342718\n",
       "2  \\n\\nAmendment No. 1 to Form 10-K\\n\\n \\n\\n    \\...  5.301764\n",
       "3  \\n\\n\\n\\n\\namzn-20201231false2020FY0001018724P3...  6.652604\n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t...  6.460655"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['avg_word'] = data['Text'].apply(lambda x: avg_word(x))\n",
    "data[['Text','avg_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then sort by the average word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nAmendment No. 1 to Form 10-K\\n\\n \\n\\n    \\...</td>\n",
       "      <td>5.301764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n...</td>\n",
       "      <td>5.522009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>\\n\\n\\n\\n\\nmsft-10k_20180630.htm\\n\\n\\n\\n\\n\\n \\n...</td>\n",
       "      <td>5.546546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>\\n\\n\\n\\n\\nmsft-10k_20190630.htm\\n\\n\\n\\n\\n\\n \\n...</td>\n",
       "      <td>5.580830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>\\n\\n\\n\\t\\n\\t\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t\\n  ...</td>\n",
       "      <td>5.622656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  avg_word\n",
       "2   \\n\\nAmendment No. 1 to Form 10-K\\n\\n \\n\\n    \\...  5.301764\n",
       "35  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n...  5.522009\n",
       "53  \\n\\n\\n\\n\\nmsft-10k_20180630.htm\\n\\n\\n\\n\\n\\n \\n...  5.546546\n",
       "52  \\n\\n\\n\\n\\nmsft-10k_20190630.htm\\n\\n\\n\\n\\n\\n \\n...  5.580830\n",
       "38  \\n\\n\\n\\t\\n\\t\\t\\n\\t\\t\\n\\t\\tDocument\\n\\t\\n\\t\\n  ...  5.622656"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['Text','avg_word']].sort_values(by='avg_word', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major component of doing analysis on text is the cleaning of the text prior to the analysis.\n",
    "\n",
    "Though this process destroys some elements of the text (sentence structure, for example), it is often necessary in order to describe a text analytically. Depending on your choice of cleaning techniques, some elements might be preserved better than others if that is of importance to your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This series of steps aims to clean up and standardize the text itself. This generally consists of removing common elements such as stopwords and punctuation but can be expanded to more detailed removals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we enforce that all of the text is lowercase. This makes it easier to match cases and sort words.\n",
    "\n",
    "Notice we are assigning our modified column back to itself. This will save our modifications to our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    goog-20201231false2020fy0001652044p7yp3yp3yp1y...\n",
       "1    document false--12-31fy20190001652044729000000...\n",
       "2    amendment no. 1 to form 10-k united states sec...\n",
       "3    amzn-20201231false2020fy0001018724p3yp1yp1yus-...\n",
       "4    document 0.5p1yp1yp1y0029000000false--12-31fy2...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'] = data['Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "data['Text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we remove all punctuation from the data. This allows us to focus on the words only as well as assist in matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    goog20201231false2020fy0001652044p7yp3yp3yp1yp...\n",
       "1    document false1231fy20190001652044729000000753...\n",
       "2    amendment no 1 to form 10k united states secur...\n",
       "3    amzn20201231false2020fy0001018724p3yp1yp1yusga...\n",
       "4    document 05p1yp1yp1y0029000000false1231fy20190...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'] = data['Text'].str.replace('[^\\w\\s]','', regex=True)\n",
    "data['Text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we remove all the numeric digits from the data. We replace digits with blank spaces, then strip all excess spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    goog        false    fy          p yp yp yp yp...\n",
       "1    document false    fy                          ...\n",
       "2    amendment no   to form   k united states secur...\n",
       "3    amzn        false    fy          p yp yp yusga...\n",
       "4    document   p yp yp y          false    fy     ...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    goog        false    fy          p yp yp yp yp...\n",
       "1    goog        false    fy          p yp yp yp yp...\n",
       "2    goog        false    fy          p yp yp yp yp...\n",
       "3    goog        false    fy          p yp yp yp yp...\n",
       "4    goog        false    fy          p yp yp yp yp...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'] = data['Text'].str.replace('[\\d]',' ', regex=True)\n",
    "data['Text'] = ' '.join(data['Text'].str.strip())\n",
    "data['Text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are words that are commonly used and do little to aid in the understanding of the content of a text. There is no universal list of stopwords and they vary on the style, time period and media from which your text came from.  Typically, people choose to remove stopwords from their data, as it adds extra clutter while the words themselves provide little to no insight as to the nature of the data.  For now, we are simply going to count them to get an idea of how many there are.\n",
    "\n",
    "For this tutorial, we will use the standard list of stopwords provided by the Natural Language Toolkit python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data['Text'] = data['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "data['Text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Frequent Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to catch common words that might have slipped through the stopword removal, we can build out a list of the most common words remaining in our text.\n",
    "\n",
    "Here we have built a list of the 10 most common words. Some of these words might actually be relevant to our analysis so it is important to be careful with this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(data['Text']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now follow the same procedure with which we removed stopwords to remove the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = list(freq.index)\n",
    "data['Text'] = data['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "data['Text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is often a more useful approach than stemming because it leverages an understanding of the word itself to convert the word back to its root word. However, this means lemmatization is less aggressive than stemming (probably a good thing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "data['Text'] = data['Text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data['Text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have a several options for cleaning and structuring our text data. The next section will focus on more advanced ways to study text analytically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section focuses on more complex methods of analyzing textual data. We will continue to work with our same DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are combinations of multiple words as they appear in the text. The N refers to the number of words captured in the list. N-grams with N=1 are referred unigrams and are just a nested list of all the words in the text. Following a similar pattern, bigrams (N=2), trigrams (N=3), etc. can be used.\n",
    "\n",
    "N-grams allow you to capture the structure of the text which can be very useful. For instance, counting the number of bigrams where \"said\" was preceded by \"he\" vs \"she\" could give you an idea of the gender breakdown of speakers in a text. However, if you make your N-grams too long, you lose the ability to make comparisons.\n",
    "\n",
    "Another concern, especially in very large data sets, is that the memory storage of N-grams scales with N (bigrams are twice as large as unigrams, for example) and the time to process the N-grams can balloon dramatically as well.\n",
    "\n",
    "All that being said, we would suggest focusing on bigrams and trigrams as useful analysis tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "n_grams = TextBlob(data['Text'][1]).ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters=[]\n",
    "for i in ['wolf', 'mouse', 'rabbit', 'children']:\n",
    "     characters.append(lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in n_grams:\n",
    "    if n[1] in characters:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "ngram_counts = Counter(ngrams(data['Text'][1].split(), 2))\n",
    "for n in ngram_counts.most_common(10):\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency is a measure of how often a term appears in a document. There are different ways to define this but the simplest is a raw count of the number of times each term appears.\n",
    "\n",
    "There are other ways of defining this including a true term frequency and a log scaled definition. All three have been implemented below but the default will the raw count definition, as it matches with the remainder of the definitions in this tutorial.\n",
    "\n",
    "|Definition|Formula|\n",
    "|---|---|\n",
    "|Raw Count|$$f_{t,d}$$|\n",
    "|Term Frequency|$$\\frac{f_{t,d}}{\\sum_{t'\\in d}f_{t',d}}$$|\n",
    "|Log Scaled|$$\\log(1+f_{t,d})$$|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Raw Count Definition\n",
    "tf1 = (data['Text'][0:5]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "\n",
    "## Term Frequency Definition\n",
    "#tf1 = (data['Text'][0:5]).apply(lambda x: (pd.value_counts(x.split(\" \")))/len(x.split(\" \"))).sum(axis = 0).reset_index() \n",
    "\n",
    "## Log Scaled Definition\n",
    "#tf1 = (data['Text'][0:10]).apply(lambda x: 1.0+np.log(pd.value_counts(x.split(\" \")))).sum(axis = 0).reset_index() \n",
    "\n",
    "tf1.columns = ['words','tf']\n",
    "tf1.sort_values(by='tf', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse Document Frequency is a measure of how common or rare a term is across multiple documents. That gives a measure of how much weight that term carries.\n",
    "\n",
    "For a more concrete analogy of this, imagine a room full of NBA players; here a 7 foot tall person wouldn't be all that shocking. However if you have a room full of kindergarten students, a 7 foot tall person would be a huge surprise.\n",
    "\n",
    "The simplest and standard definition of Inverse Document Frequency is to take the logarithm of the ratio of the number of documents containing a term to the total number of documents.\n",
    "\n",
    "$$-\\log\\frac{n_t}{N} = \\log\\frac{N}{n_t}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "    tf1.loc[i, 'idf'] = np.log(data.shape[0]/(len(data[data['Text'].str.contains(word)])))\n",
    "\n",
    "tf1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency – Inverse Document Frequency ([TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency – Inverse Document Frequency (TF-IDF) is a composite measure of both Term Frequency and Inverse Document Frequency.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf):\n",
    "\"A high weight in TF–IDF is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms\"\n",
    "\n",
    "More concisely, a high TD-IDF says that a word is very important in the documents in which it appears.\n",
    "\n",
    "There are a few weighting schemes for TF-IDF. Here we use scheme (1).\n",
    "\n",
    "|Weighting Scheme|Document Term Weight|\n",
    "|---|---|\n",
    "|(1)|$$f_{t,d}\\cdot\\log\\frac{N}{n_t}$$|\n",
    "|(2)|$$1+\\log(f_{t,d})$$|\n",
    "|(3)|$$(1+\\log(f_{t,d}))\\cdot\\log\\frac{N}{n_t}$$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1.sort_values(by='tfidf', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that the *sklearn* library has the ability to directly calculate a TD-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "data_vect = tfidf.fit_transform(data['Text'])\n",
    "\n",
    "data_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # This suppresses a warning from scikit learn that they are going to update their code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we can look at is how similar two texts are. This has a practical use when looking for plagiarism, but can also be used to compare author's styles. To do this there are a few ways we can measure similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to set up our two sets of texts. Here we have the ability to choose the size of our sets and whether we want the first n texts from our full data set or just a random sample. Keep in mind that if you want to use two dissimilar sets, you won't have a control value (something x itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First n by First n\n",
    "set1 = data[[\"Index\",'Text','Company']][:size]\n",
    "set2 = data[[\"Index\",'Text','Company']][:size]\n",
    "\n",
    "## Random X Itself\n",
    "#set1 = data[[\"Index\",'Text','Company']].sample(size)\n",
    "#set2 = set1\n",
    "\n",
    "## First n by Random\n",
    "#set1 = data[[\"Index\",'Text','Company']][:size]\n",
    "#set2 = data[[\"Index\",'Text','Company']].sample(size)\n",
    "\n",
    "## Random X Random\n",
    "#set1 = data[[\"Index\",'Text','Company']].sample(size)\n",
    "#set2 = data[[\"Index\",'Text','Company']].sample(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To let us do a \"cross join\": Every row in one gets matched to all rows in the other\n",
    "set1['key']=1 \n",
    "set2['key']=1\n",
    "\n",
    "similarity = pd.merge(set1, set2, on ='key', suffixes=('_1', '_2')).drop(\"key\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is using a metric called the [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index). This is just taking the intersection of two sets of things (in our case, words or n-grams) and dividing it by the union of those sets. This gives us a metric for understanding how the word usage compares but doesn't account for repeated words since the union and intersections just take unique words. One advantage though is that we can easily extend the single word similarity to compare bi-grams and other n-grams if we want to examine phrase usage.\n",
    "\n",
    "$$S_{J}(A,B)=\\frac{A \\cap B}{A \\cup B}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(row, n=1):\n",
    "    \n",
    "    old=row['Text_1']\n",
    "    new=row['Text_2']\n",
    "    \n",
    "    old_n_grams = [tuple(el) for el in TextBlob(old).ngrams(n)]\n",
    "    new_n_grams = [tuple(el) for el in TextBlob(new).ngrams(n)]\n",
    "        \n",
    "    union = list(set(old_n_grams) | set(new_n_grams))\n",
    "    intersection = list(set(old_n_grams) & set(new_n_grams))\n",
    "    \n",
    "    lu = len(union)\n",
    "    li = len(intersection)\n",
    "    \n",
    "    return (li/lu,li,lu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2]: # Add values to the list for the n-gram you're interested in\n",
    "    similarity['Jaccard_Index_for_{}_grams'.format(i)]=similarity.apply(lambda x: jaccard(x,i)[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity['Intersection']=similarity.apply(lambda x: jaccard(x)[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity['Union']=similarity.apply(lambda x: jaccard(x)[2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second metric we can use is [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity), however there is a catch here. Cosine similarity requires a vector for each word so we make a choice here to use term frequency. You could choose something else, inverse document frequency or tf-idf would both be good choices. Cosine similarity with a term frequency vector gives us something very similar to the Jaccard Index but accounts for word repetition. This makes it better for tracking word importance between two texts.\n",
    "\n",
    "$$S_{C}(v_1,v_2)=cos(\\theta)=\\frac{v_1\\cdot v_2}{||v_1||\\times||v_2||}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_sim(str1,str2): \n",
    "    vectors = [t for t in get_vectors([str1,str2])]\n",
    "    return cosine_similarity(vectors)\n",
    "    \n",
    "def get_vectors(slist):\n",
    "    text = [t for t in slist]\n",
    "    vectorizer = CountVectorizer(text)\n",
    "    vectorizer.fit(text)\n",
    "    return vectorizer.transform(text).toarray()\n",
    "\n",
    "def cosine(row):\n",
    "    \n",
    "    old=row['Text_1']\n",
    "    new=row['Text_2']\n",
    "    \n",
    "    return get_cosine_sim(old,new)[0,1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity['Cosine_Similarity']=similarity.apply(lambda x: cosine(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'Cosine_Similarity'\n",
    "#metric = 'Jaccard_Index_for_1_grams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = list(similarity['Index_1'].unique())\n",
    "columns = list(similarity['Index_2'].unique())\n",
    "df = pd.DataFrame(0, index=index, columns=columns)\n",
    "\n",
    "for i in df.index:\n",
    "    sub = similarity[(similarity['Index_1']==i)]\n",
    "    for col in df.columns:\n",
    "        df.loc[i,col]=sub[sub['Index_2']==col][metric].iloc[0]\n",
    "        \n",
    "plt.pcolor(df)\n",
    "plt.yticks(np.arange(0.5, len(df.index), 1), df.index)\n",
    "plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity[['Company_1','Index_1','Company_2','Index_2',metric]].sort_values(by=metric,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) a way to represent text based on the idea that similar texts will contain similar vocabulary. There is a lot to this model and we provide merely a simple implementation of it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "data_bow = bow.fit_transform(data['Text'])\n",
    "data_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment is a way of measuring the overall positivity or negativity in a given text.\n",
    "\n",
    "To do this we will use the built in sentiment function in the *TextBlob* package. This function will return the polarity and subjectivity scores for each data chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'][:5].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing on the polarity score, we are able to see the overall sentiment of each data chunk. The closer to 1 the more positive and the closer to -1 the more negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['sentiment'] = data['Text'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "data[['Text','sentiment']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have textted the sentiment scores for the first 10 chunks.\n",
    "\n",
    "Notice they tend to be positive but not exceedingly so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_1 = data[['sentiment']][:10].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have sorted and textted all of the sentiment scores for the chunks in our database.\n",
    "\n",
    "We can clearly see that most of the text data is positive but not overwhelmingly so (as seen by the long tail of the distribution). However, the parts that are negative tend to be more polarized than the positive ones (a shorter tail and sharper peak)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2 = data[['sentiment']].sort_values(by='sentiment', ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is significantly more advanced than the rest of the tutorial. This takes the TF-IDF matrix and applies a [k-means clustering algorithm](https://en.wikipedia.org/wiki/K-means_clustering). This groups the texts into clusters of similar terms from the TF-IDF matrix. This algorithm randomly seeds X \"means\", the values are then clustered into the nearest mean. The centroid of the values in each cluster then becomes the new mean and the process repeats until a convergence is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "groups = 5\n",
    "\n",
    "num_clusters = groups\n",
    "num_seeds = groups\n",
    "max_iterations = 300\n",
    "labels_color_map = {\n",
    "    0: '#20b2aa', 1: '#ff7373', 2: '#ffe4e1', 3: '#005073', 4: '#4d0404',\n",
    "    5: '#ccc0ba', 6: '#4700f9', 7: '#f6f900', 8: '#00f91d', 9: '#da8c49'\n",
    "}\n",
    "pca_num_components = 2\n",
    "tsne_num_components = 2\n",
    "\n",
    "# calculate tf-idf of texts\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',stop_words= 'english',ngram_range=(1,1))\n",
    "tf_idf_matrix = tfidf.fit_transform(data['Text'])\n",
    "\n",
    "# create k-means model with custom config\n",
    "clustering_model = KMeans(\n",
    "    n_clusters=num_clusters,\n",
    "    max_iter=max_iterations,\n",
    "    precompute_distances=\"auto\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "labels = clustering_model.fit_predict(tf_idf_matrix)\n",
    "#print(labels)\n",
    "\n",
    "X = tf_idf_matrix.todense()\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "reduced_data = PCA(n_components=pca_num_components).fit_transform(X)\n",
    "# print(reduced_data)\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "legendlist=[mpatches.Patch(color=labels_color_map[key],label=str(key))for key in labels_color_map.keys()]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for index, instance in enumerate(reduced_data):\n",
    "    #print(instance, index, labels[index])\n",
    "    pca_comp_1, pca_comp_2 = reduced_data[index]\n",
    "    color = labels_color_map[labels[index]]\n",
    "    ax.scatter(pca_comp_1, pca_comp_2, c=color)\n",
    "plt.legend(handles=legendlist)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# t-SNE plot\n",
    "#embeddings = TSNE(n_components=tsne_num_components)\n",
    "#Y = embeddings.fit_transform(X)\n",
    "#plt.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = tf1.sort_values(by='tfidf', ascending=False)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_groups = np.transpose([labels,data['Company'],data['Date']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the titles of the texts in each cluster. Keep in mind that each time you run the algorithm, the randomness in generating the initial means will result in different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(title_groups)):\n",
    "    data.loc[i,'Group'] = title_groups[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp=1\n",
    "data[data['Group']==grp]['Company']#.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(groups):\n",
    "    print(\"\")\n",
    "    print(\"#### {} ###\".format(i))\n",
    "    print(\"\")\n",
    "    for el in title_groups:\n",
    "        if el[0]==i:\n",
    "            print(\"{}\".format(el[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
